# Supported and current values of the ini file:
[global]
# The directory where the IO500 runs
datadir = /home/dpc_root/fs660
# The data directory is suffixed by a timestamp. Useful for running several IO500 tests concurrently.
timestamp-datadir = TRUE
# The result directory.
resultdir = ./results
# The result directory is suffixed by a timestamp. Useful for running several IO500 tests concurrently.
timestamp-resultdir = TRUE
# The general API for the tests (to create/delete the datadir, extra options will be passed to IOR/mdtest)
api = POSIX
# Purge the caches, this is useful for testing and needed for single node runs
drop-caches = FALSE
# Cache purging command, invoked before each I/O phase
drop-caches-cmd = sudo -n bash -c "echo 3 > /proc/sys/vm/drop_caches"
# Allocate the I/O buffers on the GPU
io-buffers-on-gpu = FALSE
# The verbosity level between 1 and 10
verbosity = 1
# Use the rules for the Student Cluster Competition
scc = FALSE

[debug]
# For a valid result, the stonewall timer must be set to the value according to the rules, it can be smaller for testing
stonewall-time = 300

[ior-easy]
# The API to be used
API = 
# Transfer size
transferSize = 512k
# Block size; must be a multiple of transferSize
# blockSize = 9920000m
blockSize = 80000m
# Create one file per process
filePerProc = TRUE
# Use unique directory per file per process
uniqueDir = TRUE
# Run this phase
run = TRUE
# The verbosity level
verbosity = 

[ior-easy-write]
# The API to be used
API = 

[mdtest-easy]
# The API to be used
API = 
# Files per proc
n = 1600000
# Run this phase
run = TRUE

[mdtest-easy-write]
# The API to be used
API = 
# Run this phase
run = TRUE

[timestamp]

[ior-hard]
# The API to be used
API = 
# Number of segments
segmentCount = 10000000
# Collective operation (for supported backends)
collective = 
# Run this phase
# run = TRUE
run = TRUE
# The verbosity level
verbosity = 

[ior-hard-write]
# The API to be used
API = 
# Collective operation (for supported backends)
collective = 

[mdtest-hard]
# The API to be used
API = 
# Files per proc
n = 1000000
# File limit per directory (MDTest -I flag) to overcome file system limitations 
files-per-dir = 
# Run this phase
run = TRUE

[mdtest-hard-write]
# The API to be used
API = 
# Run this phase
run = TRUE

[find]
# Set to an external script to perform the find phase
external-script = ./bin/cfind.sh
# Startup arguments for external scripts, some MPI's may not support this!
#external-mpi-args = mpirun -np 4
external-mpi-args = mpirun -np 10 --hostfile hostlist_find.n10
# Extra arguments for the external scripts
external-extra-args = 
# Set the number of processes for pfind/the external script
nproc = 
# Run this phase
run = TRUE
# Pfind queue length
pfind-queue-length = 10000
# Pfind Steal from next
pfind-steal-next = FALSE
# Parallelize the readdir by using hashing. Your system must support this!
pfind-parallelize-single-dir-access-using-hashing = FALSE

[ior-easy-read]
# The API to be used
API = 

[mdtest-easy-stat]
# The API to be used
API = 
# Run this phase
run = TRUE

[ior-hard-read]
# The API to be used
API = 
# Collective operation (for supported backends)
collective = 

[mdtest-hard-stat]
# The API to be used
API = 
# Run this phase
run = TRUE

[mdtest-easy-delete]
# The API to be used
API = 
# Run this phase
run = TRUE

[mdtest-hard-read]
# The API to be used
API = 
# Run this phase
run = TRUE

[mdtest-hard-delete]
# The API to be used
API = 
# Run this phase
run = TRUE

